{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and the data\n",
    "\n",
    "\n",
    "- [Text and binary](#Text-and-binary): streaming, serialization, regular expression\n",
    "- [The Web](#The-Web): XML parsing, html scraping, web frameworks, API calls\n",
    "- [Data Storage](#Data-storage): SQLite, SQL querrying, Chunking and HDF5, pytables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text manipulation is quite simplified in Python thanks to a wide variety of packages. It is generally advisable not to reinvent the wheels, so only perform quick and dirty regular expression based text parsing when it is really necessary. This is because complex RE parsing is hard to decode and test properly.\n",
    "\n",
    "### File streaming\n",
    "\n",
    "In the tutorial we exercised raw text file opening in Python. What if we want to read text from the standard input? (Useful for pipelining, and generally for saving space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use like this: cat file.txt | python script.py\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    # do suff\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text streaming involves using only the communication layer and the RAM, and not storing the data on disk immediately. When is this useful:\n",
    "- You want to use input from a dozen of super large archived FASTQ files.\n",
    "- You want to asynchronuously write to a couple of output files, in cases when you are using multuthreading or multiprocessing.\n",
    "- You want to pipe the result of a Python computation straight into some program running on another machine, another cluster node etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIrst stream into the buffer. Second stream.\n",
      "\n",
      "Inital value for read buffer\n",
      "Second read output: \n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Writing to a buffer\n",
    "output = io.StringIO()\n",
    "output.write('FIrst stream into the buffer. ')\n",
    "print('Second stream.', file=output)\n",
    "\n",
    "# Retrieve the value written\n",
    "print(output.getvalue())\n",
    "\n",
    "output.close()  # discard buffer memory\n",
    "\n",
    "# Initialize a read buffer\n",
    "input = io.StringIO('Inital value for read buffer')\n",
    "\n",
    "# Read from the buffer\n",
    "print(input.read())\n",
    "\n",
    "print(\"Second read output:\",input.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "- read a byte stream input and decode it to ascii using the io.TextIOWrapper class.\n",
    "- read a larger gzipped file via python and the gzip module and make a second version, using an OS native program (such as tar -xf or gunzip). Time the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling\n",
    "\n",
    "This in Python jargon means object serialization, a very important feature allowing you to save on disk the contents of a Python datastructure directly, in a specially compressed or sometimes binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': [1, 'two'], 'second': {'five', 3, 4}}\n"
     ]
    }
   ],
   "source": [
    "d = {'first': [1,\"two\"], 'second': set([3, 4, 'five'])}\n",
    "import pickle\n",
    "with open('dumpfile.pkl','wb') as fout:\n",
    "    pickle.dump(d, fout)\n",
    "with open('dumpfile.pkl','rb') as fin:\n",
    "    d2 = pickle.load(fin)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "A short word for JavaScript Object Notation, .json became ubiquitous as a simple data interchange format mainly in remote Web API calls and microtransactions. Json is easily loaded into native Python datastructures. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"first\": [1, \"two\"], \"second\": [3, 4, \"five\"]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#json_string = json.dumps([1, 2, 3, \"a\", \"b\", \"c\"])\n",
    "d = {'first': [1,\"two\"], 'second': [3, 4, 'five']}\n",
    "json_string = json.dumps(d)\n",
    "print(json_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feather\n",
    "\n",
    "When it comes to fast serialization between R and Python, the current champion is [Feather](https://github.com/wesm/feather). However, since any disk operation is limited by the mechanics of the disk, for extreme performance it is recommended to keep the serialized objects in memory or use SSDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(feather)\n",
    "path <- \"my_data.feather\"\n",
    "write_feather(df, path)\n",
    "df <- read_feather(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "path = 'my_data.feather'\n",
    "feather.write_dataframe(df, path)\n",
    "df = feather.read_dataframe(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and regular expressions\n",
    "\n",
    "Used for any raw text format in biology, such as (FASTA, FASTAQ, PDB, VCF, GFF, SAM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: FASTA parsing\n",
    "\n",
    "Open the file containing all peptide sequences in the human body.\n",
    "How many unknown peptides does it contain?\n",
    "How many unique genes and transcripts are in there for the unknown peptides?\n",
    "Output a tab separated file containing the gene id and transcript id for each unknown peptide.\n",
    "\n",
    "Observation:\n",
    "Usage of Biopython and pandas modules.\n",
    "\n",
    "Task:\n",
    "Order the chromosomes by the number of unknown peptides versus the total number of peptides they translate.\n",
    "\n",
    ">ENSP00000388523 pep:known chromosome:GRCh38:7:142300924:142301432:1 gene:ENSG00000226660 transcript:ENST00000455382 gene_biotype:TR_V_gene transcript_biotype:TR_V_gene\n",
    "MDTWLVCWAIFSLLKAGLTEPEVTQTPSHQVTQMGQEVILRCVPISNHLYFYWYRQILGQ\n",
    "KVEFLVSFYNNEISEKSEIFDDQFSVERPDGSNFTLKIRSTKLEDSAMYFCASSE\n",
    "\n",
    "Task:\n",
    "- Run the code below and figure out what I did. Python scripting is very often all about inheriting someone elses bloaded abandonware and making it work for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99436 28828 28828 11116 70608\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "f = open('data/Homo_sapiens.GRCh38.pep.all.fa','r')\n",
    "peptides = {}\n",
    "for l in f:\n",
    "    if l[0]=='>':\n",
    "        #print l.strip().split()\n",
    "        record = {}\n",
    "        r = l.strip('\\n').split()\n",
    "        pepid = r[0][1:]\n",
    "        record['pep'] = 1 if r[1].split(':')[1]=='known' else 0\n",
    "        record['gene'] = r[3].split(':')[1]\n",
    "        record['transcript'] = r[4].split(':')[1]\n",
    "        peptides[pepid] = record\n",
    "f.close()\n",
    "\n",
    "##using regular expressions to match all known peptides\n",
    "nupep2 = 0\n",
    "import re\n",
    "#pattern = re.compile('^>.*(known).*')\n",
    "pattern = re.compile('^>((?!known).)*$')\n",
    "with open('data/Homo_sapiens.GRCh38.pep.all.fa','r') as f:\n",
    "    for l in f:\n",
    "        if pattern.search(l) is not None: nupep2 += 1 \n",
    "\n",
    "npep = len(peptides)\n",
    "upep = set([pepid for pepid in peptides if peptides[pepid]['pep']==0]) #unknown peptides\n",
    "nunknown = len(upep)\n",
    "genes = set([peptides[pepid]['gene'] for pepid in upep])\n",
    "trans = set([peptides[pepid]['transcript'] for pepid in upep])\n",
    "print npep, nupep2, nunknown, len(genes), len(ntrans)\n",
    "\n",
    "\n",
    "with open('unknown_peptides.txt','w') as f:\n",
    "    for pepid in upep:\n",
    "        f.write('\\t'.join([pepid, peptides[pepid]['gene'], peptides[pepid]['transcript']])+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen an example of how text processing is done in Python using the standard libraries. However, you should only do this when your task is extremely unusual. For most other cases it is preferable to use a dedicated library. Most biological formats have dedicated libraries in Python, and when only available in another tool or language it is always preferable to glue a call.\n",
    "\n",
    "Example: the \"@\" character in FASTQ is also a valid confidence score. If you make a hasted script matching for \"@\" as the deliniation of a new record, you might also end up with a corrupted result.\n",
    "\n",
    "Task:\n",
    "\n",
    "Here is an example of how you should do the task above. Run this via Jupyter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/Homo_sapiens.GRCh38.pep.all.fa','r')\n",
    "from Bio import SeqIO\n",
    "fasta = SeqIO.parse(f,'fasta')\n",
    "\n",
    "i = 0\n",
    "name, sequence = fasta.id, fasta.seq.tostring()\n",
    "if len(sequence)<100 and len(sequence)>20:\n",
    "    i += 1\n",
    "    print i\n",
    "    print \"Name\",name\n",
    "    print \"Sequence\",sequence\n",
    "    if i > 5: break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Web\n",
    "\n",
    "A lot of the information today is web based, so Python has tools to help parsing the most popular web formats, web frameworks for client and server side processing, but also more mundane tasks such as web site scraping or making API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML parsing\n",
    "\n",
    "XML is a general file format used for data interchange, especially among different applications. One of the most popular use in Biology is the SBML format, that aims to store a biological model specification, no matter how specific that model may be.\n",
    "\n",
    "Task:\n",
    "- Download a curated SBML file from the BioModels database:\n",
    "http://www.ebi.ac.uk/biomodels-main/\n",
    "- Find out how many reactions the file contains.\n",
    "\n",
    "Extra task:\n",
    "- Make a simplified XML file of the reactants and their k-values for each reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.ElementTree(file='data/curated_sbml.xml')\n",
    "#tree = ET.parse(open('data/curated_sbml.xml'))\n",
    "root = tree.getroot()\n",
    "print root.tag, root.attrib\n",
    "for child in root:\n",
    "    print child.tag, child.attrib\n",
    "    for child2 in child:\n",
    "        print child2.tag, child2.attrib\n",
    "\n",
    "#print tree.write(sys.stdout)\n",
    "for elem in root.iter('reaction'):\n",
    "    print elem.tag, elem.attrib\n",
    "\n",
    "for elem in root.iter('species'):\n",
    "    print elem.tag, elem.attrib\n",
    "    print elem.get('id')\n",
    "\n",
    "print tree.findall('.//reaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xmltodict\n",
    "\n",
    "The standard library option is recommended when making a program, but it will be an overkill when having to parse a file with a quick script. \n",
    "\n",
    "Task:\n",
    "- download and install xmltodict, then list the reactions using this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "with open('data/curated_sbml.xml','r') as fd:\n",
    "    doc = xmltodict.parse(fd.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping\n",
    "\n",
    "This is concerned with automatic information processing from the Internet.\n",
    "\n",
    "Task:\n",
    "- Mine an online KEGG pathway for its reaction elements. \n",
    "\n",
    "[BeautifulSoup](http://www.pythonforbeginners.com/python-on-the-web/beautifulsoup-4-python/) is loved by hackers. Aside from html it can also parse xml.\n",
    "\n",
    "Here is a small script that will list all web anchors from Reddit main page (an anchod is a html tag normally used to provide hyperlinks and reference points inside a web page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "redditFile = urllib2.urlopen(\"http://www.reddit.com\")\n",
    "redditHtml = redditFile.read()\n",
    "redditFile.close()\n",
    "\n",
    "soup = BeautifulSoup(redditHtml)\n",
    "redditAll = soup.find_all(\"a\")\n",
    "for links in soup.find_all('a'):\n",
    "    print (links.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Frameworks \n",
    "\n",
    "As a general purpose language, Python is very popular for server side scripting. If Javascript rules as the scripting language of the web client, on the web server Python is ubiquitous due to it's fast prototyping. Only very recently Javascript started to also be popular, with frameworks like node.js.\n",
    "\n",
    "Why would this matter for you?\n",
    "- You can present your research interactively.\n",
    "- Interactivity also helps you work with your own data.\n",
    "- A web interface allows anyone to inspect your data or your findings.\n",
    "- It allows you to link your data to public datasets and the opposite.\n",
    "\n",
    "\n",
    "#### Flask\n",
    "\n",
    "Flask is a very capable microframework widely used for web development.\n",
    "\n",
    "[http://flask.pocoo.org/](http://flask.pocoo.org/)\n",
    "\n",
    "Task:\n",
    "- Run the data/flasktest.py file and open the browser at :http://0.0.0.0:5001/hello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not run this cell!\n",
    "from flask import Flask\n",
    "app = Flask(\"the_flask_module\")\n",
    "\n",
    "@app.route(\"/hello\")\n",
    "def hello_page():\n",
    "    return \"I'm a hello page\"\n",
    "\n",
    "@app.route(\"/hello/details\")\n",
    "def hello_deeper():\n",
    "    return \"I'm a details page\"\n",
    "\n",
    "app.run(host=\"0.0.0.0\", port=5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Django\n",
    "\n",
    "Worth just mentioning [Django](https://docs.djangoproject.com/en/1.7/) is a similarly popular yet more mature web framework that was amont the first to use a model-view-controller architecture wich simplifies reusability. One can write entire websites only from python code and html templates, although in general Javascript is also used for complex websites along with manual database configurations.\n",
    "\n",
    "#### Using Jupyter for web interaction\n",
    "\n",
    "While it is possible to turn Jupyter into an interactive web form with buttons and other standard widgets, we will not have time to do this, as it would suppose to learn a lot of web development concepts. \n",
    "\n",
    "However, we presented an interactive example in the Jupyter section and we will also learn how to use Python to create interactive web plots inside the plotting chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote web API calls example\n",
    "\n",
    "Getting information as fast as possible into our Python data structures is vital. Only as a last resource should one program his own downloaders and parsers. When this is not found in Python, it can be possible to call libraries from Perl or Python or access web records with specified API calls. BioPython wraps a few API calls such as Entrez resources. Entrez is a federated search engine over various NCBI and NIH resource databases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['DbList'])\n",
      "['pubmed', 'protein', 'nuccore', 'nucleotide', 'nucgss', 'nucest', 'structure', 'genome', 'annotinfo', 'assembly', 'bioproject', 'biosample', 'blastdbinfo', 'books', 'cdd', 'clinvar', 'clone', 'gap', 'gapplus', 'grasp', 'dbvar', 'epigenomics', 'gene', 'gds', 'geoprofiles', 'homologene', 'medgen', 'mesh', 'ncbisearch', 'nlmcatalog', 'omim', 'orgtrack', 'pmc', 'popset', 'probe', 'proteinclusters', 'pcassay', 'biosystems', 'pccompound', 'pcsubstance', 'pubmedhealth', 'seqannot', 'snp', 'sra', 'taxonomy', 'unigene', 'gencoll', 'gtr']\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"your@mail.here\"     # Always tell NCBI who you are\n",
    "handle = Entrez.einfo()\n",
    "#result = handle.read()\n",
    "record = Entrez.read(handle)\n",
    "print(record.keys())\n",
    "print(record[\"DbList\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[BioPython](http://biopython.org/wiki/Main_Page)**\n",
    "\n",
    "So let us for example find the exact lineage for this amazing breed of bacteria that changed both plants and atmosphere in the earlier days of our planet... As biologists that try to learn Python, I hope you will love BioPython at least as much as I do. A number of programmers created Bio::Perl which is to date containing a few more modules than BioPython, however I got the feeling the Python version is more updated. It is unfortunate that we don't have time to explore it in a great detail. \n",
    "We will use it again over the course. \n",
    "\n",
    "Aside from BioPython, web API can be ofered by virtually any website and with a little effort one can either download an Python access package or program his own. Functional annotation for example, is weakly covered in Python, but [DAVID](http://david.abcc.ncifcrf.gov/content.jsp?file=WS.html) is another API independent from BioPython.\n",
    "\n",
    "First, install with:\n",
    "\n",
    "```\n",
    "conda install -c https://conda.anaconda.org/anaconda biopython\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1142']\n",
      "dict_keys(['UpdateDate', 'ParentTaxId', 'GeneticCode', 'TaxId', 'ScientificName', 'OtherNames', 'CreateDate', 'PubDate', 'MitoGeneticCode', 'LineageEx', 'Lineage', 'Rank', 'Division'])\n",
      "cellular organisms; Bacteria; Terrabacteria group; Cyanobacteria/Melainabacteria group; Cyanobacteria; Oscillatoriophycideae; Chroococcales\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"your@mail.here\"     # Always tell NCBI who you are\n",
    "handle = Entrez.esearch(db=\"Taxonomy\", term=\"Synechocystis\")\n",
    "record = Entrez.read(handle)\n",
    "print(record[\"IdList\"])\n",
    "#assuming only one record is returned\n",
    "handle = Entrez.efetch(db=\"Taxonomy\", id=record[\"IdList\"][0], retmode=\"xml\")\n",
    "records = Entrez.read(handle)\n",
    "print(records[0].keys())\n",
    "print(records[0][\"Lineage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a RESTful API via Python\n",
    "\n",
    "\n",
    "```\n",
    "$ pip install flask flask-jsonpify flask-sqlalchemy flask-restful\n",
    "```\n",
    "Task:\n",
    "- test via browser:\n",
    "    - :6789/snps \n",
    "    - :6789/snps/snp_id\n",
    "- use a third library to test in Jupyter. Many times you will not find someone to guide you precisely so you have to figure things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from flask_restful import Resource, Api\n",
    "from sqlalchemy import create_engine\n",
    "from json import dumps\n",
    "from flask.ext.jsonpify import jsonify\n",
    "\n",
    "db_connect = create_engine('sqlite:///mylittle.db')\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "\n",
    "class SNP(Resource):\n",
    "    def get(self):\n",
    "        conn = db_connect.connect() # connect to database\n",
    "        query = conn.execute(\"select * from snps\") # This line performs query and returns json result\n",
    "        return {'snips': [i[0] for i in query.cursor.fetchall()]} # Fetches first column that is Employee ID\n",
    "\n",
    "class Gene_Name(Resource):\n",
    "    def get(self, snp_id):\n",
    "        conn = db_connect.connect()\n",
    "        query = conn.execute(\"select * from snps where Id =%d \"  %int(snp_id))\n",
    "        result = {'data': [dict(zip(tuple(query.keys()),i)) for i in query.cursor]}\n",
    "        return jsonify(result)\n",
    "        \n",
    "\n",
    "# adding two routes\n",
    "api.add_resource(SNP, '/snps')\n",
    "api.add_resource(Gene_Name, '/snps/<snp_id>')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     app.run(port='6789')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and the databases\n",
    "\n",
    "Why would you ever need to know database interaction through Python?\n",
    "\n",
    "- Almost every piece of biological or even scientific data is stored in a database.\n",
    "- Relational databases can be interogated with a very simple query language called SQL.\n",
    "- Most programs are mere interfaces to databases.\n",
    "- Stop pushing buttons, a bit of Python and a bit of SQL is all you need to bring you to the data!\n",
    "\n",
    "### SQLite\n",
    "\n",
    "This is a very simple database. Most R annotation packages to not do anything but download a SQLite database into your computers. It is faster to directly interogate it through Python than to learn how to use a package specific set of functions. \n",
    "\n",
    "The code bellow creates a test database with a table of SNPs and inserts a few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import sys\n",
    "\n",
    "snps = (\n",
    "    (1, 'Gene1', 52642),\n",
    "    (2, 'Gene2', 57127),\n",
    "    (3, 'Gene3', 9000),\n",
    "    (4, 'Gene4', 29000)\n",
    ")\n",
    "\n",
    "\n",
    "con = lite.connect('test.db')\n",
    "\n",
    "with con:\n",
    "    cur = con.cursor()    \n",
    "    cur.execute(\"DROP TABLE IF EXISTS snps\")\n",
    "    cur.execute(\"CREATE TABLE snps(Id INT, GeneSYM TEXT, NucleodidePos INT)\")\n",
    "    cur.executemany(\"INSERT INTO snps VALUES(?, ?, ?)\", snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us interogate the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Gene1', 52642)\n",
      "(2, 'Gene2', 57127)\n",
      "(3, 'Gene3', 9000)\n",
      "(4, 'Gene4', 29000)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as lite\n",
    "import sys\n",
    "\n",
    "\n",
    "con = lite.connect('test.db')\n",
    "\n",
    "with con:    \n",
    "    \n",
    "    cur = con.cursor()    \n",
    "    cur.execute(\"SELECT * FROM snps\")\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL is an interogation language that can get relatively complex and it falls out of the scope of this course. However in data science it is extremely useful to be able to operate databases because relational databases allow for very fast data access and operations, together with data compression. However there are many other database types, used predominantly in big data, such as document databases, graph databases and others, also known as NoSQL databases, and Python can bridge to them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text chunking\n",
    "\n",
    "There is no general library for chunking that I can reccomend. Text data is chunked differently than images, sounds, videos etc. Go ahead and test this multiprocessing ad-hoc example of text chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp,os\n",
    "\n",
    "def process_wrapper(chunkStart, chunkSize):\n",
    "    with open(\"input.txt\") as f:\n",
    "        f.seek(chunkStart)\n",
    "        lines = f.read(chunkSize).splitlines()\n",
    "        for line in lines:\n",
    "            process(line)\n",
    "\n",
    "def chunkify(fname,size=1024*1024):\n",
    "    fileEnd = os.path.getsize(fname)\n",
    "    with open(fname,'r') as f:\n",
    "        chunkEnd = f.tell()\n",
    "    while True:\n",
    "        chunkStart = chunkEnd\n",
    "        f.seek(size,1)\n",
    "        f.readline()\n",
    "        chunkEnd = f.tell()\n",
    "        yield chunkStart, chunkEnd - chunkStart\n",
    "        if chunkEnd > fileEnd:\n",
    "            break\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(cores)\n",
    "jobs = []\n",
    "\n",
    "#create jobs\n",
    "for chunkStart,chunkSize in chunkify(\"input.txt\"):\n",
    "    jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize)) )\n",
    "\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "#clean up\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking numerical data: HDF5, pytables\n",
    "\n",
    "For chunking numerical data, the most popular format on PC is HDF5. However on clouds there are specialized streaming libraries that are much more efficient. We will discuss the Map/Reduce paradigm at the data engineering chapters.\n",
    "\n",
    "Task:\n",
    "- Adapt the introductory code bellow to store single cell expression data. How can you improve querrying? What do you know about indexing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tables import *\n",
    "\n",
    "class Particle(IsDescription):\n",
    "    identity = StringCol(itemsize=22, dflt=\" \", pos=0)  # character String\n",
    "    idnumber = Int16Col(dflt=1, pos = 1)  # short integer\n",
    "    speed    = Float32Col(dflt=1, pos = 2)  # single-precision\n",
    "\n",
    "# Open a file in \"w\"rite mode\n",
    "fileh = open_file(\"objecttree.h5\", mode = \"w\")\n",
    "\n",
    "# Get the HDF5 root group\n",
    "root = fileh.root\n",
    "\n",
    "# Create the groups\n",
    "group1 = fileh.create_group(root, \"group1\")\n",
    "group2 = fileh.create_group(root, \"group2\")\n",
    "\n",
    "# Now, create an array in root group\n",
    "array1 = fileh.create_array(root, \"array1\", [\"string\", \"array\"], \"String array\")\n",
    "\n",
    "# Create 2 new tables in group1\n",
    "table1 = fileh.create_table(group1, \"table1\", Particle)\n",
    "table2 = fileh.create_table(\"/group2\", \"table2\", Particle)\n",
    "\n",
    "# Create the last table in group2\n",
    "array2 = fileh.create_array(\"/group1\", \"array2\", [1,2,3,4])\n",
    "\n",
    "# Now, fill the tables\n",
    "for table in (table1, table2):\n",
    "    # Get the record object associated with the table:\n",
    "    row = table.row\n",
    "\n",
    "    # Fill the table with 10 records\n",
    "    for i in xrange(10):\n",
    "        # First, assign the values to the Particle record\n",
    "        row['identity']  = 'This is particle: %2d' % (i)\n",
    "        row['idnumber'] = i\n",
    "        row['speed']  = i * 2.\n",
    "\n",
    "        # This injects the Record values\n",
    "        row.append()\n",
    "\n",
    "    # Flush the table buffers\n",
    "    table.flush()\n",
    "\n",
    "# Finally, close the file (this also will flush all the remaining buffers!)\n",
    "fileh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
